import seaborn as sns
import numpy as np

tips = sns.load_dataset("tips")
sns.regplot(x="total_bill", y="tip", data=tips)

"""
Ridge Regression derivation (lasso derivation in progress)
Cost = (y – X Beta)T (y – X Beta)  + lambda2 BetaT Beta 
= yTy – 2yT X Beta + BetaT XT X Beta + lambda2 Beta T Beta

dCost / dBeta
= -2XT y + 2 XT X Beta + 2 lambda2 Beta = 0

(XT X + lambda2 I) Beta = XT y

Beta = (XT X + lambda2 I)-1 XT y
"""

class LinearRegression:
    
    def __init__(self, l1_penalty=0, l2_penalty=0):
        self.l1_penalty = l1_penalty
        self.l2_penalty = l2_penalty
        
    def fit(self, X, y):
        self.scale_params = np.amax(X, axis=0)
        X /= self.scale_params
        self.Beta = np.dot(np.linalg.inv(np.dot(X.T, X) + \
                                         self.l2_penalty * np.ones(X.shape[1])), 
                           np.dot(X.T, y[:, None]))
    
    def predict(self, X):
        X /= self.scale_params
        return np.dot(X, self.Beta)
    
    def mean_squared_error(self, preds, y):
        return np.dot((y[:,None] - preds).T, (y[:,None] - preds)) / y.shape[0]
    

X, y = tips[['total_bill', 'size']].values, tips['tip'].values

np.random.shuffle(X), np.random.shuffle(y)

train_test_cutoff = tips.shape[0]//5 * 4
X_train, y_train = X[:train_test_cutoff,:], y[:train_test_cutoff]
X_test, y_test = X[train_test_cutoff:,:], y[train_test_cutoff:]

for l2_reg in [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 3e-1]:
    lm = LinearRegression(l2_penalty=l2_reg)
    lm.fit(X_train, y_train)
    y_pred = lm.predict(X_test)
    print(lm.mean_squared_error(y_pred, y_test))
